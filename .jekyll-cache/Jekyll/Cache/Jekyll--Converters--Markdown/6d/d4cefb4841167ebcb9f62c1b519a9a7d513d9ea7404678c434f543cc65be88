I"{<h1 id="introduction">Introduction</h1>

<p>Various SLAM algorithms such as Hector SLAM, Gmapping, Cartographer, RTAB-Map and ORB-SLAM 3 were explored to create a map of school tunnel. Data of the school tunnel were collected by a LiDAR, Cameras, and IMU. The sensor data were stored as a rosbag format to test the algorithms individually by replaying the rosbag file on ROS. Each algorithm created either occupancy grid map or point cloud map and their performance was analyzed.</p>

<h1 id="data-collection">Data Collection</h1>

<p style="text-align: center;"><img src="/assets/projects/School-Tunnel.png" width="250" height="250" /><strong><br />Route and map of school tunnel.</strong></p>
<p>The school tunnel at Northeastern University looked like the map above. The red line in the map indicated where the sensor data were collected and the algorithms had to contruct a map.</p>

<p style="text-align: center;"><img src="/assets/projects/RPLiDAR-A3.png" width="150" height="150" /><img src="/assets/projects/Realsense-D435i.png" width="150" height="150" /><img src="/assets/projects/Realsense-T265.png" width="150" height="150" /><img src="/assets/projects/VectorNav-VN100.png" width="150" height="150" /><strong><br />RPLiDAR A3, Realsense D435i and T265, and VectorNav VN100 sensors from the left.</strong></p>

<p>The sensors consisted of one LiDAR, two cameras, and one IMU. RPLiDAR A3 was a 2D planar LiDAR. Realsense D435i and T265 were a RGB-D and Stereo camera respectively. VectorNav VN100 was an IMU that contained a three-axis accelerometer, gyroscope, and magnetometer.</p>

<p style="text-align: center;"><img src="/assets/projects/Sensor-Mount-Top.png" width="400" height="400" /><img src="/assets/projects/Sensor-Mount-Front.png" width="155" height="155" /><strong><br />Top side of the cart with the LiDAR and IMU and front side with the cameras.</strong></p>

<p>The sensors were rigidly mounted on a rolling cart as the figures above to reduce noises as much as possible. The cart was pushed and traveled the tunnel following the trajectory planned in the beginning. During the sensor data collection, A laptop was reading all the data running ROS drivers for each sensor and recording them with a rosbag node.</p>

<p style="text-align: center;"><img src="/assets/projects/Tunnel-Rosbag.png" width="400" height="400" /><strong><br />Rosbag file recorded.</strong></p>

<h1 id="hector-slam">Hector SLAM</h1>
<p>There was a ROS package implementing Hector SLAM which was a LiDAR based SLAM algorithm. Only the LiDAR sensor data was used and the localization and mapping was performed by scan match. Scan matching was finding the sensor pose that minimized the error between the map and the new scan data using the Gauss-Newton leaster squares approach. This meant that feature detection or mathcning wasnâ€™t required which reduced the computational load.</p>

<p style="text-align: center;"><img src="/assets/projects/Hector-SLAM-1.png" width="400" height="400" /><img src="/assets/projects/Hector-SLAM-2.png" width="400" height="400" /><strong><br />Left: First 30 seconds of dataset showing good SLAM performance. Right: Entire map with some incorrect pose estimation.</strong></p>

<p>Hector SLAM generated and expanded the map as the cart moved along and provided fair pose information. However, during the travel in long and straight sections of the tunnel, it wasnâ€™t able to function properly since there were no distinct differences in the consecutive LiDAR scans and the cart was assumed to be stationary.</p>

<p><br />
more accurate odometry data using IMU
more advanced sensor fusion algorithm using all sensor data
better comparison analysis with a ground truth</p>
:ET