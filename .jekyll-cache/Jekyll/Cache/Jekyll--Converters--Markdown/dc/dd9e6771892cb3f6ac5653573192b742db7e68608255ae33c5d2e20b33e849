I"C(<p>Some of the contents in this page is based on the following resources:<br />
<a style="text-decoration: none;" href="/assets/projects/vehicle-detection/ML for Computer Vision EECE 5644 Final Report.pdf" target="_blank">Report <i class="fa fa-file"></i></a><br />
<a style="text-decoration: none;" href="https://universe.roboflow.com/roboflow-100/vehicles-q0x2v" target="_blank">On-road Vehicle Dataset <i class="fa fa-external-link-alt"></i></a><br />
<a style="text-decoration: none;" href="https://github.com/tae-h-yang/ml-vehicle-detection/tree/main" target="_blank">Code <i class="fa fa-code"></i></a><br /></p>

<h1 id="introduction">Introduction</h1>
<p>The principle and trend of deep learning based object detection models were reserached. Between the several models, two (RetinaNet and YOLOv5) were selected to be tested. In order to investigate the application of object detection in autonomous driving, a pre-labled on-road vehicle dataset was prepared. The dataset provided location and vehicle information of vehicle objects in each image. Accordingly, the object detection models were trained, validated, and tested. The resulting performace of each model was analyzed and presented at the end.</p>

<h1 id="overview-of-object-detection">Overview of Object Detection</h1>
<p>The task of object detection is to identify and localize objects within digital images or video frames which are basically a sequence of digital images. In order to detect multiple objects in one image or video frame, a training dataset has to provide both classification and localization information. Then, a object detection model trained with the dataset is able to properly detect where and what they were.</p>

<p>Object localization can be divided into two steps <a href="#1">[1]</a>. The first step is to generate a set of potential object proposals or candidate bounding boxes through a region proposal network (RPN). The proposals act as potential regions of interest (ROIs) that might contain objects. In the next step, bounding box coordinates of the proposals are refined to precisely fit the objects. It is achieved by bounding box regression. The model learns to predict adjustments or offsets to the candidate bounding box coordinates, adjusting their position, size, and aspect ratio.</p>

<p>Subsequently, the refined object proposals are fed into a classification network to assign a label or category to each object. The classfication network extracts meaningful features in the objects and calculates the highest probability of being mapped to one of the predefined classes.</p>

<h1 id="trend-of-object-detection">Trend of Object Detection</h1>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/Object-Detection-Trend.png" width="800" height="500" /><strong><br />Fig. 1: Trend of deep learning based object detection algorithms <a href="#2">[2]</a>.</strong></p>

<p>Since 2012, various object detection algorithms has been developed as in Fig. 1. Main purposes of them is to improve accuracy, speed, real-time performance, and the ability to detect object at various scales <a href="#2">[2]</a>. Between the algorithms, Faster R-CNN, YOLO, SSD, and RetinaNet are dominant in autonomous driving <a href="#3">[3]</a>. In this application, accuracy and responsiveness are highly significant factors because autonomous vehicles have to identify other vehicles, traffic agents and signs correctly and promptly to decide their control and path. Obviously, the dominant algorithms demonstrate exceptional performance regarding the factors and has become used in the most of the autonomous vehicles’ object detection models.</p>

<h1 id="taxonomy-of-object-detectors">Taxonomy of Object Detectors</h1>
<p>Object detectors can be classified based on their underlying network archituecture. There are two types of networks which are two-stage detectors and single-stage detectors. For example, R-CNN variants are two-stage detectors and YOLO, SSD, and RetinaNet are single-stage detectors.</p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/Object-Detector-Taxonomy.png" width="500" height="500" /><strong><br />Fig. 2: Two-stage vs. single-stage object detector architecture <a href="#3">[3]</a>.</strong></p>

<p>Two-stage detectors use region proposal methods such as selective search or RPNs with image features from feature extractor using convolutional layers as shown in Fig. 2-(a). The ROIs are collected by a ROI pooling whose purpose is to extract fixed-sized feature maps from variable-sized regions. The ROI pooling aligns and reshapes the regions proposal into a consistent format for subsequent classification and bounding box regression. Fully connected (FC) layers are the final layers that execute linear transformation with weights in the network architecture. They provided a flexible and trainable framework for capturing high-level representations and making object-level predictions based on the extracted ROIs.</p>

<p>On the other hand, single-stage detectors directly predict class probabilities and bounding boxes in a single pass through the network as shown in Fig. 2-(b). Instead of using the ROI generator, they operate on a dense grid of anchor boxes that cover the spatial locations and scales of potential objects in the input image. At every anchor box location, similar predictions as in the two-stage detectors are implemented. The anchor boxes act as priors in the FC layers and their coordinates are adjusted along with the class and bounding box predictions.</p>

<p>The single-stage detectors are generally faster than the two-stage detectors due to their direct prediction approach. However, the two-stage detectors often achieve higher accuracy, especially for small objects or in cases with complex scenes, due to their multi-stage nature and refined bounding box regression. In the following sections, some of the detectors (RetinaNet and YOLO) were further explored and tested with a pre-labeled dataset.</p>

<h1 id="on-road-vehicle-dataset">On-road Vehicle Dataset</h1>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/Vehicle-Dataset.png" width="300" height="300" /><strong><br />Fig. 3: Example of vehicle dataset.</strong></p>

<p>The vehicle dataset was obtained from Roboflow which was a computer vision platform. It consisted of images of real roads where vehicles were driving and provided 2634 train set, 966 valid set, and 458 test set. The vehicles in Fig. 3 were classified into one of the 12 classes which were <code class="language-plaintext highlighter-rouge">big bus</code>, <code class="language-plaintext highlighter-rouge">big truck</code>, <code class="language-plaintext highlighter-rouge">bus-l-</code>, <code class="language-plaintext highlighter-rouge">bus-s-</code>, <code class="language-plaintext highlighter-rouge">car</code>, <code class="language-plaintext highlighter-rouge">mid truck</code>, <code class="language-plaintext highlighter-rouge">small bus</code>, <code class="language-plaintext highlighter-rouge">small truck</code>, <code class="language-plaintext highlighter-rouge">truck-l-</code>, <code class="language-plaintext highlighter-rouge">truck-m-</code>, <code class="language-plaintext highlighter-rouge">truck-s-</code>, and <code class="language-plaintext highlighter-rouge">truck-xl-</code>.</p>

<h1 id="retinanet">RetinaNet</h1>
<p>RetinaNet is famous for its ability to address a class imbalance which refers to an uneven distribution of objects across different classes in a training dataset when detecting small objects which are common challenges for the single-stage detectors. The small objects have less features extracted, so RetinaNet uses a focal loss function during training and a separate network for classification and bounding box regression. The focal loss functions apply a modulating term to the cross entropy loss in order to focus learning on hard misclassified examples such as the small objects.</p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/RetinaNet-Diagram.png" width="400" height="400" /><strong><br />Fig. 4: RetinaNet Diagram <a href="#4">[4]</a>.</strong></p>

<p>The architecture of RetinaNet has a rich multi-scale feature pyramid from an input image and performs prediction at each scale as in Fig. 4. With such architecture, it can be semantically strong at all scales.</p>

<p>For the RetinaNet model training, Pytorch implementation of RetinaNet and Pascal VOC dataset format were used. In order to utilize sufficient computing resources, training and testing codes were run on Google Colab and can be found in this <a style="text-decoration: none;" href="https://github.com/tae-h-yang/ml-vehicle-detection/tree/main/retinanet/pytorch_retinanet" target="_blank">repo.</a><br /></p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/Pascal-VOC-1.png" width="300" height="300" /><img src="/assets/projects/vehicle-detection/Pascal-VOC-2.png" width="430" height="430" /><strong><br />Fig. 5: Example of Pascal VOC format and its bounding box <a href="#3">[3]</a>.</strong></p>

<p>The Pascal VOC format was a xml file as in Fig. 5. The Pascal VOC format files included classes and coordinates of bounding boxes for each object in the images. 50 epochs were performed for the training. One epoch was when all the training data were used. The number of epochs was relevant to a convergence of weights of the models.</p>

<h1 id="references">References</h1>
<p><a name="1"></a>[1] R. Shanmugamani, “Deep Learning for Computer Vision,” <em>Packt Publishing</em>, 2018.<br />
<a name="2"></a>[2] R. Sagar, “How the deep learning approach for object detection evolved over the years,” <em>Analyticsindiamag.com</em>, https://analyticsindiamag.com/how-the-deep-learning-approach-for-object-detection-evolved-over-the-years/ (accessed Jun. 20, 2023).<br />
<a name="3"></a>[3] A. Balasubramaniam and S. Pasricha, “Object Detection in Autonomous Vehicles: Status and Open Challenges,” <em>arXiv preprint arXiv:2201.07706</em>.<br />
<a name="4"></a>[4] A. Karaka, “Object Detection with RetinaNet,” <em>wanda.ai</em>, https://wandb.ai/site/articles/object-detection-with-retinanet (accessed Jun. 22, 2023).<br /></p>
:ET