<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Multiple Vehicle Detection Utilizing Deep Learning Algorithms - Tae Yang</title>
<meta name="description" content="Trained and tested deep learning based object detection models, RetinaNet and YOLOv5, on an on-road vehicle dataset and their performance was investigated.">


  <meta name="author" content="Tae Yang">
  
  <meta property="article:author" content="Tae Yang">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Tae Yang">
<meta property="og:title" content="Multiple Vehicle Detection Utilizing Deep Learning Algorithms">
<meta property="og:url" content="http://localhost:4000/projects/vehicle-detection/">


  <meta property="og:description" content="Trained and tested deep learning based object detection models, RetinaNet and YOLOv5, on an on-road vehicle dataset and their performance was investigated.">







  <meta property="article:published_time" content="2023-05-01T00:00:00-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/projects/vehicle-detection/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Tae Yang",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Tae Yang Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--project wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Tae Yang
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/courses/">Courses</a>
            </li><li class="masthead__menu-item">
              <a href="/projects/">Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/studies/">Studies</a>
            </li><li class="masthead__menu-item">
              <a href="/work-experiences/">Work Experiences</a>
            </li><li class="masthead__menu-item">
              <a href="/activities/">Activities</a>
            </li><li class="masthead__menu-item">
              <a href="/resume/">Resume</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/profile2.png" alt="Tae Yang" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Tae Yang</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Computer Engineering student minoring in Robotics at Northeastern University.<br /><br />Talks about Autonomous Driving, Robotics, SLAM, Computer Vision, and AI/ML</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Boston, MA</span>
        </li>
      

      
        
          
            <li><a href="mailto:yang.tae@northeastern.edu" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/tae-h-yang/" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/tae-yang/" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://www.instagram.com/hoonyy_11/" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i><span class="label">Instagram</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Multiple Vehicle Detection Utilizing Deep Learning Algorithms">
    <meta itemprop="description" content="Trained and tested deep learning based object detection models, RetinaNet and YOLOv5, on an on-road vehicle dataset and their performance was investigated.">
    <meta itemprop="datePublished" content="May 01, 2023">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Multiple Vehicle Detection Utilizing Deep Learning Algorithms
</h1>
          
        
          <p class="page__meta">May 2023 - June 2023</p>
        
          <!--  -->

        </header>
      

      <section class="page__content" itemprop="text">
        <p>This page is based on the following resources:<br />
<a style="text-decoration: none;" href="/assets/projects/vehicle-detection/ML for Computer Vision EECE 5644 Final Report.pdf" target="_blank">Report <i class="fa fa-file"></i></a><br />
<a style="text-decoration: none;" href="https://universe.roboflow.com/roboflow-100/vehicles-q0x2v" target="_blank">Dataset <i class="fa fa-external-link-alt"></i></a><br />
<a style="text-decoration: none;" href="https://github.com/tae-h-yang/ml-vehicle-detection/tree/main" target="_blank">Code <i class="fa fa-code"></i></a><br /></p>

<h1 id="introduction">Introduction</h1>
<p>The principle and trend of deep learning based object detection models were reserached. Between the several models, two (RetinaNet and YOLOv5) were selected to be tested. In order to investigate the application of object detection in autonomous driving, a pre-labled on-road vehicle dataset was prepared. The dataset provided location and vehicle information of vehicle objects in each image. Accordingly, the object detection models were trained, validated, and tested. The resulting performace of each model was analyzed and presented at the end.</p>

<h1 id="overview-of-object-detection">Overview of Object Detection</h1>
<p>The task of object detection is to identify and localize objects within digital images or video frames which are basically a sequence of digital images. In order to detect multiple objects in one image or video frame, a training dataset has to provide both classification and localization information. Then, a object detection model trained with the dataset is able to properly detect where and what they were.</p>

<p>Object localization can be divided into two steps <a href="#1">[1]</a>. The first step is to generate a set of potential object proposals or candidate bounding boxes through a region proposal network (RPN). The proposals act as potential regions of interest (ROIs) that might contain objects. In the next step, bounding box coordinates of the proposals are refined to precisely fit the objects. It is achieved by bounding box regression. The model learns to predict adjustments or offsets to the candidate bounding box coordinates, adjusting their position, size, and aspect ratio.</p>

<p>Subsequently, the refined object proposals are fed into a classification network to assign a label or category to each object. The classfication network extracts meaningful features in the objects and calculates the highest probability of being mapped to one of the predefined classes.</p>

<h1 id="trend-of-object-detection">Trend of Object Detection</h1>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/Object-Detection-Trend.png" width="800" height="500" /><strong><br />Fig. 1: Trend of deep learning based object detection algorithms <a href="#2">[2]</a>.</strong></p>

<p>Since 2012, various object detection algorithms has been developed as in Fig. 1. Main purposes of them is to improve accuracy, speed, real-time performance, and the ability to detect object at various scales <a href="#2">[2]</a>. Between the algorithms, Faster R-CNN, YOLO, SSD, and RetinaNet are dominant in autonomous driving <a href="#3">[3]</a>. In this application, accuracy and responsiveness are highly significant factors because autonomous vehicles have to identify other vehicles, traffic agents and signs correctly and promptly to decide their control and path. Obviously, the dominant algorithms demonstrate exceptional performance regarding the factors and has become used in the most of the autonomous vehicles’ object detection models.</p>

<h1 id="taxonomy-of-object-detectors">Taxonomy of Object Detectors</h1>
<p>Object detectors can be classified based on their underlying network archituecture. There are two types of networks which are two-stage detectors and single-stage detectors. For example, R-CNN variants are two-stage detectors and YOLO, SSD, and RetinaNet are single-stage detectors.</p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/Object-Detector-Taxonomy.png" width="500" height="500" /><strong><br />Fig. 2: Two-stage vs. single-stage object detector architecture <a href="#3">[3]</a>.</strong></p>

<p>Two-stage detectors use region proposal methods such as selective search or RPNs with image features from feature extractor using convolutional layers as shown in Fig. 2-(a). The ROIs are collected by a ROI pooling whose purpose is to extract fixed-sized feature maps from variable-sized regions. The ROI pooling aligns and reshapes the regions proposal into a consistent format for subsequent classification and bounding box regression. Fully connected (FC) layers are the final layers that execute linear transformation with weights in the network architecture. They provided a flexible and trainable framework for capturing high-level representations and making object-level predictions based on the extracted ROIs.</p>

<p>On the other hand, single-stage detectors directly predict class probabilities and bounding boxes in a single pass through the network as shown in Fig. 2-(b). Instead of using the ROI generator, they operate on a dense grid of anchor boxes that cover the spatial locations and scales of potential objects in the input image. At every anchor box location, similar predictions as in the two-stage detectors are implemented. The anchor boxes act as priors in the FC layers and their coordinates are adjusted along with the class and bounding box predictions.</p>

<p>The single-stage detectors are generally faster than the two-stage detectors due to their direct prediction approach. However, the two-stage detectors often achieve higher accuracy, especially for small objects or in cases with complex scenes, due to their multi-stage nature and refined bounding box regression. In the following sections, some of the detectors (RetinaNet and YOLO) were further explored and tested with a pre-labeled dataset.</p>

<h1 id="on-road-vehicle-dataset">On-road Vehicle Dataset</h1>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/Vehicle-Dataset.png" width="300" height="300" /><strong><br />Fig. 3: Example of vehicle dataset.</strong></p>

<p>The vehicle dataset was obtained from Roboflow which was a computer vision platform. It consisted of images of real roads where vehicles were driving and provided 2634 train set, 966 valid set, and 458 test set. The vehicles in Fig. 3 were classified into one of the 12 classes which were <code class="language-plaintext highlighter-rouge">big bus</code>, <code class="language-plaintext highlighter-rouge">big truck</code>, <code class="language-plaintext highlighter-rouge">bus-l-</code>, <code class="language-plaintext highlighter-rouge">bus-s-</code>, <code class="language-plaintext highlighter-rouge">car</code>, <code class="language-plaintext highlighter-rouge">mid truck</code>, <code class="language-plaintext highlighter-rouge">small bus</code>, <code class="language-plaintext highlighter-rouge">small truck</code>, <code class="language-plaintext highlighter-rouge">truck-l-</code>, <code class="language-plaintext highlighter-rouge">truck-m-</code>, <code class="language-plaintext highlighter-rouge">truck-s-</code>, and <code class="language-plaintext highlighter-rouge">truck-xl-</code>.</p>

<h1 id="retinanet">RetinaNet</h1>
<p>RetinaNet is famous for its ability to address a class imbalance which refers to an uneven distribution of objects across different classes in a training dataset when detecting small objects which are common challenges for the single-stage detectors. The small objects have less features extracted, so RetinaNet uses a focal loss function during training and a separate network for classification and bounding box regression. The focal loss functions apply a modulating term to the cross entropy loss in order to focus learning on hard misclassified examples such as the small objects.</p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/RetinaNet-Diagram.png" width="400" height="400" /><strong><br />Fig. 4: RetinaNet Diagram <a href="#4">[4]</a>.</strong></p>

<p>The architecture of RetinaNet has a rich multi-scale feature pyramid from an input image and performs prediction at each scale as in Fig. 4. With such architecture, it can be semantically strong at all scales.</p>

<p>For the RetinaNet model training, Pytorch implementation of RetinaNet and Pascal VOC dataset format were used. In order to utilize sufficient computing resources, training and testing codes were run on Google Colab and can be found in this <a style="text-decoration: none;" href="https://github.com/tae-h-yang/ml-vehicle-detection/tree/main/retinanet/pytorch-retinanet" target="_blank">repo.</a><br /></p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/Pascal-VOC-1.png" width="300" height="300" /><img src="/assets/projects/vehicle-detection/Pascal-VOC-2.png" width="430" height="430" /><strong><br />Fig. 5: Example of Pascal VOC format and its bounding box <a href="#3">[3]</a>.</strong></p>

<p>The Pascal VOC format was a xml file as in Fig. 5. The Pascal VOC format files included classes and coordinates of bounding boxes for each object in the images. 50 epochs were performed for the training. One epoch was when all the training data were used. The number of epochs was relevant to a convergence of weights of the models.</p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/RetinaNet-Result.png" width="600" height="600" /><strong><br />Fig. 6: Example of testing results from RetinaNet model.</strong></p>

<p>The training result showed a classification loss of 0.112, box regression loss of 0.0385, and validation loss of 0.515. The classification and box regression were trained satisfactorily, but the validation loss was high which meant there was an overfitting during the training. Thus, the training results in Fig. 6 demonstrated some misclassification and duplicate bounding boxes on a single object.</p>

<h1 id="yolov5">YOLOv5</h1>
<p>YOLO stands for You Only Look Once to emphasize its speed in object detection. It has many variants such as v1, v2, v3, and so forth. For the testing, YOLOv5 was selected since it was one of the latest versions and proved its performance on many datasets throughout open source communities. YOLOv5 is famous for proposing further data augmentation and loss calculation improvements. Data augmentation is deliberately transforming training images by cropping, resizing, and flipping to increase the diversity of the dataset. This improves the model’s generalization ability and robustness. Additionally, auto-learning bounding box anchors are featured in the detector to adapt to a given dataset. Anchor boxes are usually fixed boxes for calculating bounding boxes. Auto-learning bounding box anchors are adaptive to datasets in order to enable fast converging bounding box calculation.</p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/YOLOv5-Diagram.png" width="500" height="500" /><strong><br />Fig. 7: YOLOv5 Diagram <a href="#5">[5]</a>.</strong></p>

<p>The architecture of the detector is similar to that of RetinaNet having a rich feature pyramid to generalize to objects better on different sizes and scales and predicting classes and bounding boxes at each neck layer in Fig 7.</p>

<p>For the YOLOv5 model training, YOLOv5 model from <a style="text-decoration: none;" href="https://github.com/ultralytics/yolov5" target="_blank">Ultralytics</a> and TXT annotations dataset format were used. Training and testing codes were run on Google Colab as well and can be found in this <a style="text-decoration: none;" href="https://github.com/tae-h-yang/ml-vehicle-detection/tree/main/yolov5" target="_blank">repo.</a><br /></p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/TXT-1.png" width="300" height="300" /><img src="/assets/projects/vehicle-detection/TXT-2.png" width="338" height="338" /><strong><br />Fig. 8: Example of TXT annotation dataset <a href="#3">[3]</a>.</strong></p>

<p>Unlike the Pascal VOC format, TXT annotation format included class types and center coordinates of bounding boxes with their widths and heights. Again 50 epochs were executed for the same dataset.</p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/YOLOv5-Confusion-Matrix.png" width="500" height="500" /><strong><br />Fig. 9: Confusion matrix of YOLOv5 model on vehicle dataset.</strong></p>

<p>The YOLOv5 model from Ultralytics provided not only training losses, but also various training results such as the confusion matrix in Fig. 9. It was able to make correct detections on most of the classes with a classification loss of 0.0069 and box regression of 0.032. Its validation loss was 0.036.</p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/YOLOv5-Result.png" width="600" height="600" /><strong><br />Fig. 10: Example of testing results of YOLOv5 model.</strong></p>

<p>The YOLOv5 detected objects very accurately on the testing images as shown in Fig 10. Most of the vehicles were classified correctly and bounding boxes were fitting to them accurately. Even some vehicles that were partly occluded by other vehicles were detected showing the strong inference of the detector.</p>

<h1 id="conclusion">Conclusion</h1>

<p style="text-align: center;"><strong>Table 1: Performance comparison between RetinaNet and YOLOv5.<br /></strong></p>

<table>
  <thead>
    <tr>
      <th>Object Detector</th>
      <th>mAP</th>
      <th>Inference Rate (fps)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RetinaNet</td>
      <td>10.6%</td>
      <td>71</td>
    </tr>
    <tr>
      <td>YOLOv5</td>
      <td>41.1%</td>
      <td>129</td>
    </tr>
  </tbody>
</table>

<p>The main performance indicators were mAP (mean Average Precision) and inference rate. mAP was a commonly used metric as it combined the concepts of precision and recall to assess the accuracy and effectiveness of an object detector. It measured how well the detector localized and classified objects across different classes and different levels of confidence thresholds. A higher mAP indicated better object detection performance. Inference rate was literally how many images a detector could process in a second. There was an overfitting issue with RetinaNet and it resulted in worse mAP than that of YOLOv5. However, they both achieved high inference rates as their architectures were single-stage detector. As long as the overfitting issue with RetinaNet was resolved, it was confirmed that why the two object detection models were popular in Autonomous Driving due to their accuracy and responsiveness.</p>

<h1 id="future-work">Future Work</h1>
<p>In order to resolve the overfitting issue with RetinaNet, different training parameters can be tried and the dataset should be reviewed to see if there is any class imbalance. Additionally, several other stratagies such as early stopping, pruning, regularization, and ensebmling could be tried.</p>

<p style="text-align: center;"><img src="/assets/projects/vehicle-detection/Object-Detector-Comparison.png" width="500" height="500" /><strong><br />Fig. 11: Comparision between various object detectors that are commonly used in Autonomous Driving <a href="#3">[3]</a>.</strong></p>

<p>The performance comparision in Fig. 11 was used as a reference and trying each of the object detectors was desired to provide a thorough comparision especially depending on their architectures. For single-stage detectors, RetinaNet and YOLOv5 were selected and for two-stage detectors, Fast R-CNN and Faster R-CNN were selected. However, given the limited computing resource, it was taking too long time to train the two-stage models on Google Colab. The estimated training time was about 8 hours with 50 epochs and Google Colab was disconnected from time to time before finishing its task. Therefore, the two models can be tested with better GPU resources and reducing the testing image size and checking the right version of the required librarie such as <code class="language-plaintext highlighter-rouge">tensorflow</code> can be attempted.</p>

<h1 id="references">References</h1>
<p><a name="1"></a>[1] R. Shanmugamani, “Deep Learning for Computer Vision,” <em>Packt Publishing</em>, 2018.<br />
<a name="2"></a>[2] R. Sagar, “How the deep learning approach for object detection evolved over the years,” <em>Analyticsindiamag.com</em>, https://analyticsindiamag.com/how-the-deep-learning-approach-for-object-detection-evolved-over-the-years/ (accessed Jun. 20, 2023).<br />
<a name="3"></a>[3] A. Balasubramaniam and S. Pasricha, “Object Detection in Autonomous Vehicles: Status and Open Challenges,” <em>arXiv preprint arXiv:2201.07706</em>.<br />
<a name="4"></a>[4] A. Karaka, “Object Detection with RetinaNet,” <em>wanda.ai</em>, https://wandb.ai/site/articles/object-detection-with-retinanet (accessed Jun. 22, 2023).<br />
<a name="5"></a>[5] A. Bochkovskiy, “YOLOv4: Optimal Speed and Accuracy of Object Detection,” <em>arXiv preprint arXiv:2004.10934v1</em>.<br /></p>

        
      </section>

      <footer class="page__meta">
        
        


      </footer>

      

      
  <nav class="pagination">
    
      <a href="/projects/underwater-localization/" class="pagination--pager" title="UUV (Underwater Unmanned Vehicle) Localization
">Previous</a>
    
    
      <a href="/projects/trajectory-converter/" class="pagination--pager" title="Image to Robot Drawing Trajectory Converter
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <!-- <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div> -->

<div class="page__footer-copyright">&copy; 2025 Tae Yang. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
